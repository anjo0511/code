
```{r include=F}
library(kernlab)
```


# Task 2

In this task we are to solve the handwritten digits and build a classifier. To do this we train a kernel SVM where the kernel function is vanilladot and rbfdot for task 2.1 and 2.2 respectively. By k-fold cross-validation on the training data we evaluate different values of the slackness parameters, C, 
in 2.1 and 2.2 and kernel parameter sigma in 2.2. After optimal C and sigma is found we train a final model using these parameters and test their performance on a test set of the data. 


## Overview of the observations

We start by investigating if certain digits are over represented as we 
hypothesize that this would lead to skewed data and training sets, resulting 
in a skewed SVM.

```{r}
df= train
table(df[, 1])
```

```{r fig.width=4, fig.height=4, fig.align='center'}
barplot(round(table(df[,1]),3)*100
        , main = "Data distribution before balancing"
        , xlab = "Digits"
        , ylab = "no of observations"
        , ylim = c(0,130000))
```

After having found that this is the case we select an equal representation 
figure for each digit. We can see that the number 8 has the lowest representation and  occurs 542 times (about half of that of 1 and 0). According to our reasoning we should thus sample 542 of each of the other digits for our augmented data set as well then. 

## Balancing the dataset to eqully represented digit observations

The function below selects randomly a set of 542 images for each digit so that each class is equally represented and returns the if of the selected observations.

```{r}
ReSampling = function(howMany=542){
    set.seed(2018)
    reSampling = c()
    for (i in seq(10)){
        IDs = sample(which(df[,1]==i-1)
               ,howMany, replace = F)
        reSampling = append(reSampling,IDs)
    }
    return(reSampling)
}
newSampleID= ReSampling()
```

## Spliting up the data in training and test set

Now we are able to make a new data set which is balanced and from that divide the data set intro training and test set. We split 80% respective 20%. Down we can see the dimensions of the old and new data frames.
```{r}
newdf = df
newdf  = newdf[newSampleID,]
dim(df)
dim(newdf)
```
```{r}
nbObs = nrow(newdf)
set.seed(2018)
trainID = sample(1:nbObs,nbObs*0.8, replace = F)
testID = (1:nbObs)
testID = testID[-trainID]
```


## Task 2.1

In this sub task we train svm using the vanilladot kernel and estimating the optimal slackness parameter C. We selected a list of potential hyper parameter valuesfor each we build model using kvsm() function with a vanilladot kernel. At once we also performed a 10-fold cross validation, save CV-error for model parameter evaluation.

```{r eval=F}
C =c(0.001,0.1,1,10,100)
cvError = NULL 

for (i in 1:length(C)){
    set.seed(2018)
    svm1 = ksvm(V1~.
              , data= newdf[trainID,]
              , kernel = "vanilladot"
              , kpar = list()
              , type = 'C-svc'
              , C = C[i]
              , cross = 10)
    
    cvError[i] = cross(svm1)
}
```

Here we have the results of the model represented by their cross validation errors. The plot down below shows as well the pattern in which the lowest cross cross validation error is given by a C = 0.1, the second index in the C-list.

```{r, echo=F}
cvError
```

```{r fig.align='center',fig.width=4,fig.height=3, echo=F}
plot(cvError, col = "orange", type="b",lwd=2.5, pch = 18,ylab="10-Fold Cross Validation Error",xlab="Index of the C parameter")
grid(nx=40, ny=40)
```

## Making predictions with the test set

We first obtain the index of C that corresponded to the lowers *cv-error*, then we train test the hyper parameter with our C value and the test set. 

```{r eval=F}
minCvError = which.min(cvError)
set.seed(2018)
svm2 = ksvm(V1~.
            , data= newdf[-testID,]
            , kernel = "vanilladot"
            , kpar = list()
            , type = 'C-svc'
            , C = C[minCvError])

acc= mean(predict(svm2, newdf[testID,-1])== newdf[testID,1])
```

Tuning of hyper parameter suggests C = 0.1, as this corresponds to the lowest 10-fold cross-validation error of ~0.053. The final model gives an accuracy of 95.4% as shown below.

```{r echo=F}
cat("Accuracy: ",acc)
```

## Task 2.2, train svm using the rbf kernel and estimate optimal slackness parameter C and kernel parameter sigma.

For each pair of parameters in the data frame with pairs of C and sigma we build model with kernel function rbfdot and perform 10-fold cross-validation. Then we save CV-error into assigned column of a data frame together with corresponding value of sigma and C. 

For this task we selected a range in which both hyper parameters were going to be tested then made our model and selected a second range of parameters which performed best to see if we could improve our first fit.

The first range of selected parameters were a C = 0.001,0.1,1,10,100 and sigma= 0.1,0.01,0.001 which by all possible permutations yielded 15 models with the following combinations. 

### First set of hyper parameters
```{r echo=F}
testParam1 = expand.grid(C = c(0.001
                              ,0.1
                              ,1
                              ,10
                              ,100)
                        , sigma = c(0.1
                                    ,0.01
                                    ,0.001))
print("Testset 1")
testParam1
```

### Parameter Seeking

Now that we have defined the parameters we want to scrutinize we made a function which makes a model of each test parameter pair, performs 10 fold cv and predicts with the test set to yield accuracy. Accuracy is performed on the test set while cross validation is performed on the training set.

```{r}
parameterSeeking = function(testParam){
    cvErrorRBF = NULL
    acc = NULL
    for (row in 1:nrow(testParam)){
        cat("Making Model: ",row,"\n")
        var = testParam[row,]
        rbfdot1=rbfdot(sigma = var$sigma)
        set.seed(2018)
        svm = ksvm(V1~.
            , data= newdf[trainID,]
            , kernel = rbfdot1
            , type = 'C-svc'
            , C = var$C
            , cross = 10)
        
        cvErrorRBF[row] = cross(svm)
        cat("cvError: ",cross(svm),"\n")
        pred = predict(svm
               , newdf[testID,-1])
        acc[row] = mean(pred==newdf[testID,1])
        cat("Accuracy: ",acc[row],"\n")
        cat("---------------------------\n")
    }
 return(list(cvErrorRBF,acc))   
}
```


```{r include=F}
plotACCorCV = function(list){
    cvErrorRBF = list[[1]]
    acc = list[[2]]
    
    par(mfrow= c(1,2))
    plot(cvErrorRBF, type="b",lwd=2, col="brown"
         , ylab="10-fold Cross Validation Error")
    grid(nx=40, ny=40)
    points(which.min(cvErrorRBF)
            , cvErrorRBF[which.min(cvErrorRBF)]
            , cex=2,pch=18)
    plot(acc, type="b",lwd=2, col="lightblue"
         , ylab = "Accuracy of prediction")
    grid(nx=40, ny=40)
    points(which.max(acc)
            , acc[which.max(acc)]
            , cex=2,pch=18)
    par(mfrow= c(1,1))
}
```


```{r eval=F}
resultsOfFirstParamRange = parameterSeeking(testParam1)
```

#### Results of from the first set of hyper parameters  

The first results of the parameter hunt suggested the lowest cross validation error in consistency with higest accuracy of the indexes 14 and 15 of the parameter list. Overall the results where a bit varying but a general trend could be seen. To get a better picture of the results a plot was made i which the x-axis represents the index of the parameter set 1 and on the y axis is both the cross validation error (left) and accuracy (right). Down below we show the values that obtained the best results. This values will be scrutinize a bit more in the next parameter set in order to see if the model could be improved. 

```{r echo=F}
testParam1[14:15,]
```

```{r fig.align='center',fig.width=8,fig.height=4, echo=F}
plotACCorCV(resultsOfFirstParamRange)
```


### Second set of hyper parameters 

The second range of hyper parameters were C=10,100,150 and sigma 0.01,0.001,0.0001 by with all possible hyper parameters yielded 9 models to test.
```{r echo=F}
testParam2 = expand.grid(C = c(10
                              ,100
                              ,150)
                        , sigma = c(0.01
                                    ,0.001
                                    ,0.0001))
print("Testset 2")
testParam
```

#### Results of from the second set of hyper parameters  

As can be observed in the picture below the absolutely lowest cross validation error is obtained not only at one position but rather a range of combinations from our second set of test parameters, index 4 to 6 yields collectively the best results. In the figure to the right the same can be observed but this time in terms of accuracy. The optimal hyper parameters corresponding to these values are:
```{r echo=F}
testParam[4:6,]
```
This indirectly suggests that the value for sigma has greater impact on the regression being in a more enclosed range than C for this task.

```{r fig.align='center',fig.width=8,fig.height=4, echo=F}
plotACCorCV(resultsOfSecondParamRange)
```
